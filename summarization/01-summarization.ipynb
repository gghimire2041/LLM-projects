{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement of the Problem/Requirements\n",
    "1) Write an application to summarize the given text\n",
    "2) The summary should be free of industrial jargons\n",
    "3) Need to use any LLM from OpenAI\n",
    "4) Do not expose the OpenAI API Key\n",
    "5) Prepare a well documented jupyter notebook as well as script file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import openai\n",
    "# tiktoken is easy to use when it comes to tokenizing text and managing tokenized data\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONA_PROMPT = \"You are to act as an expert project manager.\"\n",
    "\n",
    "SECTION_PROMPT = f\"{PERSONA_PROMPT} Paraphrase each thought into bullet point statements. Do not include an intro or conclusion.\"\n",
    "TOPIC_PROMPT = f\"{PERSONA_PROMPT} Separate the following notes into sections by topic. Do not change the wording or order of notes.\"\n",
    "SUMMARY_PROMPT = f\"{PERSONA_PROMPT} Summarize the following meeting notes in Key Takeaways and Action Items. Key Takeaways and Action Items should not repeat each other.\"\n",
    "\n",
    "TEMPERATURE = 0\n",
    "OVERLAP = 50\n",
    "SECTION_RESPONSE_MAX_TOKENS = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up OpenAI API credentials and model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "if API_KEY is None:\n",
    "    print(\"Error: OPENAI_ORG_ID and OPENAI_API_KEY environment variables must be set.\")\n",
    "    sys.exit(1)\n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_text(input_file):\n",
    "    try:\n",
    "        with open(input_file, 'r') as f:\n",
    "            text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: file '{input_file}' not found.\")\n",
    "        sys.exit(1)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting for processing the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Process text file and create summaries using OpenAI.\")\n",
    "    parser.add_argument(\"input_file\", help=\"The input text file to process.\")\n",
    "    parser.add_argument(\"-o\", \"--output_file\", nargs=\"?\",\n",
    "                        help=\"The output file where the results will be saved. If omitted, the output file will be named the same as the input file, but appended with '_output' and always have a '.txt' extension.\")\n",
    "    parser.add_argument(\"-j\", \"--jargon_file\", nargs=\"?\", const=\"jargon.txt\",\n",
    "                        help=\"Replace jargon terms before processing text. Will check for jargon.txt in the current working directory unless another file location is specified.\")\n",
    "    parser.add_argument(\"-t\", \"--topics\", nargs=\"?\", const=\"prompt\",\n",
    "                        help=\"Sort notes by topic. Provide a comma-separated list of topics or use 'auto' to automatically generate topics. Default is 'prompt' which will ask for the list at runtime.\")\n",
    "    parser.add_argument(\"-s\", \"--summary\", action=\"store_true\",\n",
    "                        help=\"Generate a summary of the notes and include in the output file.\")\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input_text(text):\n",
    "    # Remove timestamps\n",
    "    text = re.sub(\n",
    "        r'\\d{2}:\\d{2}:\\d{2}.\\d{3} --> \\d{2}:\\d{2}:\\d{2}.\\d{3}\\n', '', text)\n",
    "    # Remove blank lines\n",
    "    text = '\\n'.join([line for line in text.split('\\n') if line.strip()])\n",
    "    # Remove VTT tags\n",
    "    text = re.sub(r'<v [^>]+>', '', text)\n",
    "    text = re.sub(r'</v>', '', text)\n",
    "    # Remove whitespace and new lines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace industrial jargon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_jargon(text,jargon_file):\n",
    "    if jargon_file is None:\n",
    "        # Skip replacing jargon\n",
    "        return text\n",
    "    \n",
    "    # Check if jargon.txt file exists\n",
    "    if os.path.isfile(jargon_file):\n",
    "        # Read jargon strings and replacements from file\n",
    "        with open(jargon_file, 'r') as f:\n",
    "            jargon_pairs = [tuple(line.strip().split(','))\n",
    "                            for line in f.readlines()]\n",
    "\n",
    "            # Validate jargon pairs format\n",
    "            for pair in jargon_pairs:\n",
    "                if len(pair) != 2:\n",
    "                    raise ValueError(\n",
    "                        'Incorrect format in jargon.txt file. Each line should contain two strings separated by a comma.')\n",
    "\n",
    "            # Replace jargon strings with their replacements in text\n",
    "            for jargon, replacement in jargon_pairs:\n",
    "                text = text.replace(jargon, replacement)\n",
    "    else:\n",
    "        print('jargon.txt file not found. Skipping jargon replacement...')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_model(prompt,max_tokens):\n",
    "    try:\n",
    "        # Call the openai model with the section as input\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=MODEL,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=max_tokens,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "    except openai.Error as e:\n",
    "        print(\n",
    "            f\"Error: OpenAI API call failed with status code {e.status_code} and message: {e.message}\")\n",
    "        sys.exit(1)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sections(text):\n",
    "    print(\n",
    "        f\"\\nProcesing input text by section using the following prompt...\\n\\\"{SECTION_PROMPT}\\\"\\n\")\n",
    "\n",
    "    intro_text = f\"{SECTION_PROMPT}###\"\n",
    "    outro_text = '###'\n",
    "\n",
    "    n_sections = 0\n",
    "    answers = []\n",
    "\n",
    "    # Determine section length\n",
    "    section_prompt_tokens = enc.encode(intro_text + outro_text)\n",
    "    system_prompt_tokens = enc.encode(SYSTEM_PROMPT)\n",
    "    tokenized_text = enc.encode(text)\n",
    "    section_length = 4000 - SECTION_RESPONSE_MAX_TOKENS - \\\n",
    "        len(section_prompt_tokens) - len(system_prompt_tokens)\n",
    "\n",
    "    for i in range(0, len(tokenized_text), section_length - OVERLAP):\n",
    "        section_tokens = tokenized_text[i:i+section_length]\n",
    "        section = enc.decode(section_tokens)\n",
    "        section = intro_text + section + outro_text\n",
    "\n",
    "        answer = call_openai_model(section,SECTION_RESPONSE_MAX_TOKENS)\n",
    "\n",
    "        # Filter out lines that don't start with \"-\" and are not blank\n",
    "        filtered_lines = []\n",
    "        for line in answer.split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"-\") or line:\n",
    "                filtered_lines.append(line)\n",
    "        filtered_answer = \"\\n\".join(filtered_lines)\n",
    "\n",
    "        answers.append(filtered_answer)\n",
    "        print(filtered_answer)\n",
    "        n_sections += 1\n",
    "\n",
    "    # Combine the answers into a single string\n",
    "    full_notes = '\\n'.join(answers)\n",
    "    return full_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort text by topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_topic(full_notes, topics):\n",
    "    if topics is None:\n",
    "        # Skip sorting notes\n",
    "        return full_notes\n",
    "    elif topics == \"auto\":\n",
    "        # Let GPT pick the topics to sort by\n",
    "        combined_topic_prompt = TOPIC_PROMPT\n",
    "    else:\n",
    "        # Use the specified topics to sort by\n",
    "        combined_topic_prompt = f\"{TOPIC_PROMPT} Topics: {topics}\"\n",
    "\n",
    "    print(\n",
    "        f\"\\nSorting text by topic using the following prompt...\\n\\\"{combined_topic_prompt}\\\"\\n\")\n",
    "\n",
    "    topic_intro_text = f\"{combined_topic_prompt}###\"\n",
    "    topic_outro_text = \"###\"\n",
    "\n",
    "    # Determine max summary length\n",
    "    topic_prompt_tokens = enc.encode(topic_intro_text + topic_outro_text)\n",
    "    system_prompt_tokens = enc.encode(SYSTEM_PROMPT)\n",
    "    full_notes_tokens = enc.encode(full_notes)\n",
    "    topic_length = 4000 - len(full_notes_tokens) - \\\n",
    "        len(topic_prompt_tokens) - len(system_prompt_tokens)\n",
    "\n",
    "    topic_input = topic_intro_text + full_notes + topic_outro_text\n",
    "\n",
    "    sorted_notes = call_openai_model(topic_input,topic_length)\n",
    "\n",
    "    # Remove leading and trailing blank lines\n",
    "    while sorted_notes.startswith(\"\\n\"):\n",
    "        sorted_notes = sorted_notes[1:]\n",
    "    while sorted_notes.endswith(\"\\n\"):\n",
    "        sorted_notes = sorted_notes[:-1]\n",
    "\n",
    "    print(f\"{sorted_notes}\")\n",
    "    return sorted_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_summary(sorted_notes):\n",
    "    print(\n",
    "        f\"\\nSummarizing text using the following prompt...\\n\\\"{SUMMARY_PROMPT}\\\"\\n\")\n",
    "    summary_intro_text = f\"{SUMMARY_PROMPT}###\"\n",
    "    summary_outro_text = '###'\n",
    "\n",
    "    # Determine max summary length\n",
    "    summary_prompt_tokens = enc.encode(summary_intro_text + summary_outro_text)\n",
    "    tokenized_text = enc.encode(sorted_notes)\n",
    "    system_prompt_tokens = enc.encode(SYSTEM_PROMPT)\n",
    "    summary_length = 4000 - len(tokenized_text) - \\\n",
    "        len(summary_prompt_tokens) - len(system_prompt_tokens)\n",
    "\n",
    "    summary_input = summary_intro_text + sorted_notes + summary_outro_text\n",
    "\n",
    "    summary_notes = call_openai_model(summary_input,summary_length)\n",
    "\n",
    "    # Remove leading and trailing blank lines\n",
    "    while summary_notes.startswith(\"\\n\"):\n",
    "        summary_notes = summary_notes[1:]\n",
    "    while summary_notes.endswith(\"\\n\"):\n",
    "        summary_notes = summary_notes[:-1]\n",
    "\n",
    "    print(f\"{summary_notes}\")\n",
    "    return summary_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output_to_file(input_file, output_file, combined_notes):\n",
    "    if output_file is None:\n",
    "        # Generate output file name from input file name\n",
    "        input_file_base = os.path.splitext(input_file)[0]\n",
    "        output_file = input_file_base + \"_output.txt\"\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(combined_notes)\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error: could not write output to file {output_file}: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Assign values based on user input\n",
    "    args = parse_arguments()\n",
    "    input_file = args.input_file\n",
    "    jargon_file = args.jargon_file\n",
    "    output_file = args.output_file\n",
    "    topics = args.topics\n",
    "    generate_summary = args.summary\n",
    "\n",
    "    # Prompt user for topics if needed\n",
    "    if(topics == \"prompt\"):\n",
    "        topics = input(\"What topics would you like to sort notes by? \")\n",
    "\n",
    "    # Read input text from file\n",
    "    input_text = get_input_text(input_file)\n",
    "\n",
    "    # Process the input file and tokenize it\n",
    "    clean_text = clean_input_text(input_text)\n",
    "    clean_text = replace_jargon(clean_text,jargon_file)\n",
    "\n",
    "    # Process sections of text\n",
    "    full_notes = process_sections(clean_text)\n",
    "\n",
    "    # Sort notes by topic (if requested)\n",
    "    sorted_notes = sort_by_topic(full_notes, topics)\n",
    "\n",
    "    # Summarize notes and combine with sorted_notes if requested\n",
    "    if (generate_summary):\n",
    "        summary_notes = process_summary(sorted_notes)\n",
    "\n",
    "        # Combine summary and notes\n",
    "        combined_notes = f\"{summary_notes}\\n\\nNotes:\\n{sorted_notes}\"\n",
    "    else:\n",
    "        combined_notes = sorted_notes\n",
    "\n",
    "    # Write to file\n",
    "    output_file = write_output_to_file(input_file, output_file, combined_notes)\n",
    "\n",
    "    print(f'\\nYour summary of notes have been written to \"{output_file}\".')\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
